{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch教程\n",
    "## 0. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from torch.utils.data import Dataset, DataLoader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Tensor Basics\n",
    "### 1.1 创建Tensor\n",
    "* ```torch.empty(size)```：没有初始化的tensor\n",
    "* ```torch.rand(size)```：随机初始化，[0, 1]\n",
    "* ```torch.zeros(size)```：全0\n",
    "* ```torch.tensor(tuple or list)```：从python的dict或者tuple创建\n",
    "* ```torch.from_numpy(np.array)```：从numpy的array创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.empty(size): uninitiallized\n",
    "x = torch.empty(3, 5)\n",
    "x = torch.empty_like(x)\n",
    "\n",
    "# torch.rand(size): random numbers [0, 1]\n",
    "x = torch.rand(3, 5)\n",
    "x = torch.rand(x.size())\n",
    "x = torch.rand_like(x)\n",
    "\n",
    "# torch.zeros(size), fill with 0\n",
    "# torch.ones(size), fill with 1\n",
    "x = torch.ones(3, 5)\n",
    "x = torch.ones_like(x)\n",
    "\n",
    "# construct from python data\n",
    "x = torch.tensor([1, 2, 3])\n",
    "\n",
    "# numpy <-> cpu tensor\n",
    "# If the Tensor is on the CPU (not the GPU), both objects will share the same memory location, so changing one will also change the other\n",
    "a = np.ones((3, 5))\n",
    "b = torch.from_numpy(a)\n",
    "c = b.numpy()\n",
    "\n",
    "# cpu <-> gpu\n",
    "x_cpu = torch.rand(3, 5)\n",
    "x_gpu = x_cpu.to(torch.device(\"cuda\"))\n",
    "x_gpu = torch.rand(3, 5, device=torch.device(\"cuda\"))\n",
    "\n",
    "# check size\n",
    "# check data type\n",
    "# check requires_grad\n",
    "print(x.size())\n",
    "print(x.dtype)\n",
    "print(x.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 基本运算\n",
    "* 加减除：```+, -, /, add, sub, div```\n",
    "* 乘法：\n",
    "  * ```*, mul```：点乘\n",
    "  * ```mm, matmul```：矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 0], [0, 1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0.1, 1], [1, 0.1]], dtype=torch.float32)\n",
    "\n",
    "# elementwise addition, substraction, division\n",
    "print(x + y)\n",
    "print(torch.add(x, y))\n",
    "print(x - y)\n",
    "print(torch.sub(x, y))\n",
    "print(x / y)\n",
    "print(torch.div(x, y))\n",
    "\n",
    "# multiplication\n",
    "# elementwise\n",
    "print(x * y)\n",
    "print(torch.mul(x, y))\n",
    "# matrix product\n",
    "print(torch.mm(x, y))\n",
    "print(torch.matmul(x, y))  # broadcast supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autograd\n",
    "* 误差还没有反向传播时，grad默认为None，第一次传播之后grad变成Tensor类的实例化对象，而且会一直累加，因此需要手动zero_\n",
    "* 不需要追踪某个tensor的梯度时：\n",
    "  * ```with torch.no_grad():```\n",
    "  * ```x.detach_()```\n",
    "  * ```x.requires_grad_(False)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.ones(3, 3, requires_grad=True)\n",
    "input = torch.tensor([[0.1, 0.7], [0.2, 0.9], [0.3, 0.6]], dtype=torch.float32)\n",
    "label = torch.tensor([0, 1])\n",
    "\n",
    "epoch_loop = tqdm(range(0, 100))\n",
    "for epoch in epoch_loop:\n",
    "    output = torch.sigmoid(torch.matmul(weights, input).sum(dim=0))\n",
    "    loss = nn.L1Loss(reduction='mean')(output, label)\n",
    "    \n",
    "    if weights.grad is not None:\n",
    "        weights.grad.zero_()\n",
    "    loss.backward()  # backward() accumulates the gradient for this tensor into .grad attribute\n",
    "    with torch.no_grad():\n",
    "        weights -= 0.1 * weights.grad\n",
    "    \n",
    "    epoch_loop.set_description(\"Epoch: %d\" % (epoch + 1))\n",
    "    epoch_loop.set_postfix_str(\"loss is: %.6f\" % loss.item())\n",
    "\n",
    "\n",
    "print(weights)\n",
    "weights.requires_grad_(False)  # weights.detach_()\n",
    "print(weights)\n",
    "print(output.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Pipeline\n",
    "* Design model\n",
    "* Construct loss and optimizer\n",
    "* Training loop:\n",
    "  * forward pass: compute prediction and loss\n",
    "  * backward pass: compute gradients\n",
    "  * update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy implementation\n",
    "class NumpyImplementation(object):\n",
    "    def __init__(self, w, lr, epochs=10):\n",
    "        self.w = w\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.w * x\n",
    "    \n",
    "    def loss(self, y_predicted, y):\n",
    "        return ((y_predicted - y) ** 2).mean()\n",
    "\n",
    "    def grad(self, x, y_predicted, y):\n",
    "        return np.dot(2 * x, (y_predicted - y)) / len(x)\n",
    "\n",
    "    def update_weights(self, grad):\n",
    "        self.w -= self.lr * grad\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def train(self, x, y):\n",
    "        print(\"numpy implementation\".center(90, '-'))\n",
    "        print(f\"before training, f(5) = {self(5):.3f}\")\n",
    "\n",
    "        epoch_loop = tqdm(range(0, self.epochs))\n",
    "        for epoch in epoch_loop:\n",
    "            y_pred = self(x)\n",
    "            l = self.loss(y_pred, y)\n",
    "            g = self.grad(x, y_pred, y)\n",
    "            self.update_weights(g)\n",
    "\n",
    "            epoch_loop.set_description(f'epoch: {epoch + 1}')\n",
    "            epoch_loop.set_postfix_str(f'loss = {l:.8f}, w = {self.w:.2f}')\n",
    "\n",
    "        print(f'after training, f(5) = {self(5):.3f}')\n",
    "\n",
    "# pytorch implementation: using autograd to obtain grad\n",
    "class TorchImplementation1(NumpyImplementation):\n",
    "    def __init__(self, w, lr, epochs=10):\n",
    "        super(TorchImplementation1, self).__init__(w, lr, epochs)\n",
    "        self.w = torch.tensor(self.w, requires_grad=True)\n",
    "\n",
    "    def train(self, x, y):\n",
    "        x = torch.from_numpy(x)\n",
    "        y = torch.from_numpy(y)\n",
    "\n",
    "        print(\"pytorch implementation version 1\".center(90, '-'))\n",
    "        print(f\"before training, f(5) = {self(5):.3f}\")\n",
    "\n",
    "        epoch_loop = tqdm(range(0, self.epochs))\n",
    "        for epoch in epoch_loop:\n",
    "            y_pred = self(x)\n",
    "            l = self.loss(y_pred, y)\n",
    "            if self.w.grad is not None:\n",
    "                self.w.grad.zero_()\n",
    "            l.backward()\n",
    "            with torch.no_grad():\n",
    "                self.update_weights(self.w.grad)\n",
    "\n",
    "            epoch_loop.set_description(f'epoch: {epoch + 1}')\n",
    "            epoch_loop.set_postfix_str(f'loss = {l:.8f}, w = {self.w:.2f}')\n",
    "\n",
    "        print(f'after training, f(5) = {self(5):.3f}')\n",
    "\n",
    "# pytorch implementation: using optimizer to update weights\n",
    "class TorchImplementation2(NumpyImplementation):\n",
    "    def __init__(self, w, lr, epochs=10):\n",
    "        super(TorchImplementation2, self).__init__(w, lr, epochs)\n",
    "        self.w = torch.tensor(self.w, requires_grad=True)\n",
    "        self.optimizer = torch.optim.SGD([self.w], self.lr)  # 需要更新的权重必须是可迭代的\n",
    "\n",
    "    def train(self, x, y):\n",
    "        x = torch.from_numpy(x)\n",
    "        y = torch.from_numpy(y)\n",
    "\n",
    "        print(\"pytorch implementation version 2\".center(90, '-'))\n",
    "        print(f\"before training, f(5) = {self(5):.3f}\")\n",
    "\n",
    "        epoch_loop = tqdm(range(0, self.epochs))\n",
    "        for epoch in epoch_loop:\n",
    "            y_pred = self(x)\n",
    "            l = self.loss(y_pred, y)\n",
    "            self.optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_loop.set_description(f'epoch: {epoch + 1}')\n",
    "            epoch_loop.set_postfix_str(f'loss = {l:.8f}, w = {self.w:.2f}')\n",
    "\n",
    "        print(f'after training, f(5) = {self(5):.3f}')\n",
    "\n",
    "\n",
    "# pytorch implementation: using nn.Linear to build network and nn.MSELoss to compute loss\n",
    "class TorchImplementation3(object):\n",
    "    def __init__(self, lr, epochs=10):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.model = nn.Linear(1, 1, bias=False)\n",
    "        nn.init.constant_(self.model.weight, 0)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), self.lr)\n",
    "\n",
    "    def train(self, x, y):\n",
    "        x = torch.from_numpy(x).unsqueeze(-1)\n",
    "        y = torch.from_numpy(y).unsqueeze(-1)\n",
    "\n",
    "        print(\"pytorch implementation version 3\".center(90, '-'))\n",
    "        print(f\"before training, f(5) = {self.model(torch.tensor([5.0])).item():.3f}\")\n",
    "\n",
    "        epoch_loop = tqdm(range(0, self.epochs))\n",
    "        for epoch in epoch_loop:\n",
    "            y_pred = self.model(x)\n",
    "            l = self.loss(y_pred, y)\n",
    "            self.optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_loop.set_description(f'epoch: {epoch + 1}')\n",
    "            epoch_loop.set_postfix_str(f'loss = {l.item():.8f}, w = {self.model.state_dict()[\"weight\"].item():.2f}')\n",
    "\n",
    "        print(f'after training, f(5) = {self.model(torch.tensor([5.0])).item():.3f}')\n",
    "\n",
    "\n",
    "x = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epoch = 20\n",
    "\n",
    "ni = NumpyImplementation(0.0, learning_rate, num_epoch)\n",
    "ni.train(x, y)\n",
    "\n",
    "ti1 = TorchImplementation1(0.0, learning_rate, num_epoch)\n",
    "ti1.train(x, y)\n",
    "\n",
    "ti2 = TorchImplementation2(0.0, learning_rate, num_epoch)\n",
    "ti2.train(x, y)\n",
    "\n",
    "ti3 = TorchImplementation3(learning_rate, num_epoch)\n",
    "ti3.train(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=10, random_state=4)\n",
    "x = torch.from_numpy(x_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32)).unsqueeze(-1)\n",
    "\n",
    "model = nn.Linear(x.shape[1], y.shape[1])\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epoch_loop = tqdm(range(0, 2000))\n",
    "for epoch in epoch_loop:\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        epoch_loop.set_description(f\"epoch: {(epoch + 1):d}\")\n",
    "        epoch_loop.set_postfix_str(f\"loss: {loss.item():.4f}\")\n",
    "\n",
    "y_pred = model(x).detach().cpu().numpy()\n",
    "plt.plot(x_numpy, y_numpy, \"ro\")\n",
    "plt.plot(x_numpy, y_pred, \"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "x, y = bc.data, bc.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=4)\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "x_train = torch.from_numpy(x_train.astype(np.float32))\n",
    "x_test = torch.from_numpy(x_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32)).unsqueeze(-1)\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32)).unsqueeze(-1)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(x_train.shape[1], 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "epoch_loop = tqdm(range(0, 100))\n",
    "for epoch in epoch_loop:\n",
    "    y_pred = model(x_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x_test)\n",
    "            y_pred_cls = y_pred.round()\n",
    "            acc = y_pred_cls.eq(y_test).sum() / (y_test.shape[0])\n",
    "        epoch_loop.set_description(f'epoch: {epoch + 1}')\n",
    "        epoch_loop.set_postfix_str(f'loss = {loss.item():.4f}, acc = {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class LogisticDataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        bc = datasets.load_breast_cancer()\n",
    "        x, y = bc.data, bc.target\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=4)\n",
    "\n",
    "        sc = StandardScaler()\n",
    "        x_train = sc.fit_transform(x_train)\n",
    "        x_test = sc.transform(x_test)\n",
    "\n",
    "        self.x_train = torch.from_numpy(x_train.astype(np.float32))\n",
    "        self.x_test = torch.from_numpy(x_test.astype(np.float32))\n",
    "        self.y_train = torch.from_numpy(y_train.astype(np.float32)).unsqueeze(-1)\n",
    "        self.y_test = torch.from_numpy(y_test.astype(np.float32)).unsqueeze(-1)\n",
    "        self.train = train\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.x_train)\n",
    "        else:\n",
    "            return len(self.x_test)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            return self.x_train[index], self.y_train[index]\n",
    "        else:\n",
    "            return self.x_test[index], self.y_test[index]\n",
    "\n",
    "train_dataset = LogisticDataset(train=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_dataset = LogisticDataset(train=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(30, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion= nn.BCELoss()\n",
    "\n",
    "epoch_loop = tqdm(range(0, 10), leave=True)\n",
    "for epoch in epoch_loop:\n",
    "    num_batches = len(train_dataloader)\n",
    "    epoch_per_batch = 1.0 / num_batches\n",
    "\n",
    "    for i, (data, target) in enumerate(train_dataloader):\n",
    "        epoch += epoch_per_batch\n",
    "        epoch_loop.set_description(f'epoch: {epoch:.3f}')\n",
    "\n",
    "        y_pred = model(data)\n",
    "        loss = criterion(y_pred, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            epoch_loop.set_postfix_str(f'loss = {loss.item():.4f}')\n",
    "    if round(epoch) % 10 == 0:\n",
    "        num_corrects = 0\n",
    "        num_samples = 0\n",
    "        for i, (data, target) in enumerate(test_dataloader):\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(data)\n",
    "                y_pred_cls = y_pred.round()\n",
    "                num_corrects += y_pred_cls.eq(target).sum()\n",
    "                num_samples += y_pred_cls.shape[0]\n",
    "            acc = num_corrects / num_samples\n",
    "            epoch_loop.set_postfix_str(f'loss = {loss.item():.4f}, acc = {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Softmax & Crossentropy\n",
    "* PyTorch默认的```nn.CrossEntropyLoss```，等同于```nn.LogSoftmax + nn.NLLLoss```，因此不需要在网络的最后使用```torch.softmax```，并且label的shape是```（n_samples, )```，不是one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def cross_entropy_loss(x, y):\n",
    "    return -np.dot(y, np.log(x))\n",
    "\n",
    "out = np.array([2.0, 1.0, 0.1], dtype=np.float32)\n",
    "label = np.array([1, 0, 0], dtype=np.float32)\n",
    "pred = softmax(out)\n",
    "print(cross_entropy_loss(pred, label))\n",
    "\n",
    "out = torch.tensor([[2.0, 1.0, 0.1], [1.0, 2.0, 0.1]])  # shape: n_sample * n_cls\n",
    "label = torch.tensor([0, 1])  # shape: n_sample\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(criterion(out, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tensorboard\n",
    "* global_step：可以用于显示同一个tag下不同step的图片，随着epoch的增加，显示不同的预测结果\n",
    "* add_image：tensor或者array，shape：c * h * w\n",
    "* ```tensorboard --logdir ./runs/```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "from einops import rearrange\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "if not os.path.exists(\"./runs\"):\n",
    "    os.mkdir(\"./runs\")\n",
    "\n",
    "writer = SummaryWriter(\"./runs\")\n",
    "\n",
    "# matplotlib.pyplot.figure\n",
    "fig = plt.figure(1)\n",
    "plt.plot([0, 1, 2], [2, 3, 4], '-')\n",
    "writer.add_figure(\"figure\", fig)\n",
    "\n",
    "# image: c * h * w\n",
    "img = rearrange(cv.imread(\"./materials/panda.jpg\"), 'h w c -> c h w')\n",
    "img2 = rearrange(cv.imread(\"./materials/panda2.jpg\"), 'h w c -> c h w')\n",
    "writer.add_image(\"images\", img, global_step=0)\n",
    "writer.add_image(\"images\", img2, global_step=1)\n",
    "\n",
    "# model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.BatchNorm1d(10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "input = torch.rand(4, 10)\n",
    "writer.add_graph(model, input_to_model=input)\n",
    "\n",
    "for epoch in range(0, 10):\n",
    "    writer.add_scalar(\"loss\", 2 * epoch + 1, epoch)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Distributed Traning\n",
    "* 采用```DistributedDataParallel```训练时，如果从头训练负载相对均衡，而从checkpoint开始训练就出现负载不均衡，可以采用```state = torch.load('xxx.pth', map=torch.device('cpu'))```解决\n",
    "* 保存模型时，注意加```if dist.get_rank() == 0```来保证只在一个线程里面保存，不然后面load的时候会出错\n",
    "* 只需要在一个线程里面执行的操作，比如打印误差，日志等，也可以用```if dist.get_rank() == 0```来实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. index_add & scatter_add\n",
    "* ```x.index_add_(dim, index, source)```：将source中的每一个向量，分别加在x的不同向量上，具体加在x的哪一个向量上看index，具体计算过程如下：\n",
    "  * ```x[index[i], ...] += source[i, ...]  # index[i]所在位置取决于dim```\n",
    "* ```self_tensor.scatter_add_(dim, index_tensor, other_tensor)```：将other_tensor中的数据，按照index_tensor中的索引位置，添加至self_tensor中\n",
    "  * ```self_tensor[index[i][j], j] += other_tensor[i, j]  # index[i][j]所在位置取决于dim```\n",
    "* index_add & scatter_add底层实现是不确定性算法，即便设置了随机种子和cudnn，仍然无法保证可重复性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = torch.tensor([0, 1, 1], dtype=torch.int64)\n",
    "x = torch.zeros(3, 4, dtype=torch.float32)\n",
    "print(x)\n",
    "\n",
    "print(\"index_add\".center(40, '-'))\n",
    "source = torch.rand_like(x)\n",
    "print(source)\n",
    "print(x.index_add(0, index, source))\n",
    "\n",
    "print(\"scatter_add\".center(40, '-'))\n",
    "source = torch.rand_like(index.unsqueeze(0), dtype=torch.float32)\n",
    "print(source)\n",
    "print(x.scatter_add(0, index.unsqueeze(0), source))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ordering of different layers\n",
    "-> CONV/FC -> BatchNorm -> ReLu(or other activation) -> Dropout -> CONV/FC ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPWithBatchNormAndDropout(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob=0.5):\n",
    "        super(MLPWithBatchNormAndDropout, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the MLP model\n",
    "input_size = 784  # Example input size for MNIST dataset\n",
    "hidden_size = 256\n",
    "output_size = 10\n",
    "dropout_prob = 0.5  # Probability for dropout\n",
    "model = MLPWithBatchNormAndDropout(input_size, hidden_size, output_size, dropout_prob)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a94075a2e62db5dc98a7ce177b0aa497782b90b7701cb2e0b55d059aa447695"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
